{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1907f5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use single GPU to avoid DataParallel issues)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f785da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess dataset\n",
    "def load_dataset(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please check the path and file name.\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read {file_path}. Check delimiter or encoding (e.g., try delimiter=';' or encoding='utf-8'). Error: {str(e)}\")\n",
    "    \n",
    "    if 'text' not in df.columns or 'label' not in df.columns:\n",
    "        possible_label_cols = [col for col in df.columns if 'label' in col.lower() or 'sentiment' in col.lower()]\n",
    "        raise ValueError(\n",
    "            f\"The dataset at {file_path} must contain 'text' and 'label' columns. \"\n",
    "            f\"Found: {list(df.columns)}. Possible label columns: {possible_label_cols}. \"\n",
    "            f\"Consider renaming the label column (e.g., df.rename(columns={{'{possible_label_cols[0]}': 'label'}})).\"\n",
    "        )\n",
    "    \n",
    "    # Check for NaN labels\n",
    "    nan_count = df['label'].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: Found {nan_count} rows with NaN labels in {file_path}.\")\n",
    "        if nan_count == len(df):\n",
    "            raise ValueError(\n",
    "                f\"All labels in {file_path} are NaN. Expected labels: 0, 1, 2 for positive, negative, neutral. \"\n",
    "                f\"Check the CSV content or regenerate the dataset.\"\n",
    "            )\n",
    "        print(\"Removing rows with NaN labels.\")\n",
    "        df = df.dropna(subset=['label'])\n",
    "    \n",
    "    # Check if dataset is empty\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"After removing NaN labels, no data remains in {file_path}. Please provide a dataset with valid labels.\")\n",
    "    \n",
    "    # Check for valid labels\n",
    "    unique_labels = df['label'].unique()\n",
    "    if len(unique_labels) < 2:\n",
    "        raise ValueError(\n",
    "            f\"Dataset at {file_path} has fewer than 2 unique labels: {unique_labels}. \"\n",
    "            f\"Expected at least 2 of [0, 1, 2] for positive, negative, neutral.\"\n",
    "        )\n",
    "    \n",
    "    return df[['text', 'label']]\n",
    "\n",
    "# Define paths\n",
    "home_dir = os.path.expanduser(\"~\")  # Expands ~ to /home/hagos\n",
    "amharic_data_path = os.path.join(home_dir, \"amharic_data.csv\")\n",
    "tigrinya_data_path = os.path.join(home_dir, \"tigrinya_data.csv\")\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    amharic_df = load_dataset(amharic_data_path)\n",
    "    tigrinya_df = load_dataset(tigrinya_data_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    print(\"Please ensure the dataset files are in the home directory (~) and run again.\")\n",
    "    raise\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"Please check the dataset for valid 'text' and 'label' columns and non-NaN labels.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inspect dataset statistics\n",
    "print(\"Amharic dataset size:\", len(amharic_df))\n",
    "print(\"Amharic label distribution:\\n\", amharic_df['label'].value_counts())\n",
    "print(\"Tigrinya dataset size:\", len(tigrinya_df))\n",
    "print(\"Tigrinya label distribution:\\n\", tigrinya_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encode string labels to numerical values\n",
    "'''\n",
    "\n",
    "def encode_labels(df, label_column='label'):\n",
    "    # Define fixed mapping for expected labels\n",
    "    label2id = {\n",
    "        'positive': 1, 'negative': -1, 'neutral': 0,\n",
    "        '1': 1, '1': -1, '0': 0,\n",
    "        1: 1, -1: -1, 0: 0\n",
    "    }\n",
    "    id2label = {1: 'positive', -1: 'negative', 0: 'neutral'}\n",
    "    \n",
    "    # Map labels to numerical values\n",
    "    df['label'] = df[label_column].map(label2id)\n",
    "    \n",
    "    # Check for unmapped labels\n",
    "    if df['label'].isna().any():\n",
    "        unmapped = df[df['label'].isna()][label_column].unique()\n",
    "        raise ValueError(\n",
    "            f\"Found unmapped labels: {unmapped}. Expected: [-1, 0, 1] or ['positive', 'negative', 'neutral'].\"\n",
    "        )\n",
    "    \n",
    "    # Validate labels are numerical and in range\n",
    "    if not pd.api.types.is_numeric_dtype(df['label']):\n",
    "        raise ValueError(f\"Label encoding failed. 'label' column contains non-numerical values: {df['label'].unique()}\")\n",
    "    \n",
    "    num_labels = 3  # Fixed for positive, negative, neutral\n",
    "    if not all(df['label'].isin(range(num_labels))):\n",
    "        raise ValueError(f\"Labels must be in range [-1, {num_labels-1}]. Found: {df['label'].unique()}\")\n",
    "    \n",
    "    # Ensure types are correct for transformers\n",
    "    label2id = {str(k): int(v) for k, v in label2id.items()}\n",
    "    id2label = {int(k): str(v) for k, v in id2label.items()}\n",
    "    \n",
    "    return df, label2id, id2label, num_labels\n",
    "\n",
    "# Encode labels\n",
    "try:\n",
    "    amharic_df, amharic_label2id, amharic_id2label, amharic_num_labels = encode_labels(amharic_df)\n",
    "    tigrinya_df, tigrinya_label2id, tigrinya_id2label, tigrinya_num_labels = encode_labels(tigrinya_df)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"Please ensure all labels are valid (0, 1, 2 or 'positive', 'negative', 'neutral').\")\n",
    "    raise\n",
    "\n",
    "# Print label mappings and number of labels\n",
    "print(\"Amharic label mapping:\", amharic_label2id)\n",
    "print(\"Tigrinya label mapping:\", tigrinya_label2id)\n",
    "print(\"Amharic num_labels:\", amharic_num_labels)\n",
    "print(\"Tigrinya num_labels:\", tigrinya_num_labels)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "amharic_dataset = Dataset.from_pandas(amharic_df)\n",
    "tigrinya_dataset = Dataset.from_pandas(tigrinya_df)\n",
    "''' \n",
    "def encode_labels(df, label_column='label'):\n",
    "    # Define fixed mapping for expected labels\n",
    "    label2id = {\n",
    "        'positive': 1, 'negative': -1, 'neutral': 0,\n",
    "        '1': 1, '-1': -1, '0': 0,\n",
    "        1: 1, -1: -1, 0: 0\n",
    "    }\n",
    "    id2label = {1: 'positive', -1: 'negative', 0: 'neutral'}\n",
    "    \n",
    "    # Map labels to numerical values\n",
    "    df['label'] = df[label_column].map(label2id)\n",
    "    \n",
    "    # Check for unmapped labels\n",
    "    if df['label'].isna().any():\n",
    "        unmapped = df[df['label'].isna()][label_column].unique()\n",
    "        raise ValueError(\n",
    "            f\"Found unmapped labels: {unmapped}. Expected: [-1, 0, 1] or ['positive', 'negative', 'neutral'].\"\n",
    "        )\n",
    "    \n",
    "    # Validate labels are numerical and in range\n",
    "    if not pd.api.types.is_numeric_dtype(df['label']):\n",
    "        raise ValueError(f\"Label encoding failed. 'label' column contains non-numerical values: {df['label'].unique()}\")\n",
    "    \n",
    "    valid_labels = {-1, 0, 1}  # Define valid label set\n",
    "    if not all(df['label'].isin(valid_labels)):\n",
    "        raise ValueError(f\"Labels must be in [-1, 0, 1]. Found: {df['label'].unique()}\")\n",
    "    \n",
    "    # Ensure types are correct for transformers\n",
    "    label2id = {str(k): int(v) for k, v in label2id.items()}\n",
    "    id2label = {int(k): str(v) for k, v in id2label.items()}\n",
    "    \n",
    "    num_labels = 3  # Fixed for positive, negative, neutral\n",
    "    return df, label2id, id2label, num_labels\n",
    "\n",
    "# Encode labels\n",
    "try:\n",
    "    amharic_df, amharic_label2id, amharic_id2label, amharic_num_labels = encode_labels(amharic_df)\n",
    "    tigrinya_df, tigrinya_label2id, tigrinya_id2label, tigrinya_num_labels = encode_labels(tigrinya_df)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    print(\"Please ensure all labels are valid (-1, 0, 1 or 'positive', 'negative', 'neutral').\")\n",
    "    raise\n",
    "\n",
    "# Print label mappings and number of labels\n",
    "print(\"Amharic label mapping:\", amharic_label2id)\n",
    "print(\"Tigrinya label mapping:\", tigrinya_label2id)\n",
    "print(\"Amharic num_labels:\", amharic_num_labels)\n",
    "print(\"Tigrinya num_labels:\", tigrinya_num_labels)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "amharic_dataset = Dataset.from_pandas(amharic_df)\n",
    "tigrinya_dataset = Dataset.from_pandas(tigrinya_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "amharic_tokenized = amharic_dataset.map(tokenize_function, batched=True)\n",
    "tigrinya_tokenized = tigrinya_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "amharic_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tigrinya_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Split Amharic dataset for training and validation\n",
    "amharic_split = amharic_tokenized.train_test_split(test_size=0.2, seed=42)\n",
    "amharic_train = amharic_split[\"train\"]\n",
    "amharic_val = amharic_split[\"test\"]\n",
    "\n",
    "# Validate labels in training and validation sets\n",
    "train_labels = np.unique(amharic_train['label'])\n",
    "val_labels = np.unique(amharic_val['label'])\n",
    "print(\"Amharic train labels:\", train_labels)\n",
    "print(\"Amharic val labels:\", val_labels)\n",
    "if len(train_labels) != amharic_num_labels:\n",
    "    print(f\"Warning: Only {len(train_labels)}/{amharic_num_labels} labels in training set. Consider increasing test_size or collecting more data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f39270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize model\n",
    "# Note: Warning about uninitialized weights is expected\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=amharic_num_labels,\n",
    "    id2label=amharic_id2label,\n",
    "    label2id=amharic_label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fine-tune on Amharic data\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"amharic_model\"),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"logs\"),\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=amharic_train,\n",
    "    eval_dataset=amharic_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning on Amharic data...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"amharic_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"amharic_model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a18703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Transfer learning to Tigrinya\n",
    "# Split Tigrinya dataset\n",
    "tigrinya_split = tigrinya_tokenized.train_test_split(test_size=0.2, seed=42)\n",
    "tigrinya_train = tigrinya_split[\"train\"]\n",
    "tigrinya_val = tigrinya_split[\"test\"]\n",
    "\n",
    "# Validate Tigrinya labels\n",
    "tigrinya_train_labels = np.unique(tigrinya_train['label'])\n",
    "tigrinya_val_labels = np.unique(tigrinya_val['label'])\n",
    "print(\"Tigrinya train labels:\", tigrinya_train_labels)\n",
    "print(\"Tigrinya val labels:\", tigrinya_val_labels)\n",
    "\n",
    "# Update training arguments for Tigrinya\n",
    "training_args.output_dir = os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"tigrinya_model\")\n",
    "training_args.num_train_epochs = 3\n",
    "training_args.per_device_train_batch_size = 8\n",
    "training_args.per_device_eval_batch_size = 8\n",
    "training_args.learning_rate = 2e-5\n",
    "training_args.eval_strategy = \"epoch\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tigrinya_train,\n",
    "    eval_dataset=tigrinya_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning on Tigrinya data...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"tigrinya_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(home_dir, \"AM_Tig_Transfer_Learning\", \"tigrinya_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Evaluate on Tigrinya test set\n",
    "print(\"Evaluating on Tigrinya validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
